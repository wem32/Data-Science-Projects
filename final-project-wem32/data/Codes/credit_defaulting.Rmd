---
title: "Credit Default's Classification"
author: "Wabi Mposo"
date: "August 6, 2018"
output: html_document
---

## Introduction

These codes seek to predict credit defaulting on the basis of some charasterics, which are independent variables in the dataset. 

## loading packages
```{r, eval = FALSE}

library(modelr)
library(tidyverse)
library(ggplot2)
library(randomForest)
library(readxl)
library(caret)
library(e1071)
library(gbm)

```

## Importing data

```{r, eval = FALSE}

credit_c.d <- read_excel("data/Unprocessed/default of credit card clients_1.xlsx") %>%
  mutate(default_nm=as.factor(default_nm))

View(head(credit_c.d,10))

```

## Data Cleaning

Basically, I wanted to transform the data into a time-series format from its original format. 

```{r, eval=F}

table1 <- credit_c.d %>%
  gather(key = "pay_period", value = "pay_amt", starts_with("pay")) %>% 
  transmute(pay_period =pay_period, pay_amt = pay_amt)


table2 <- credit_c.d %>%
  gather(key = "bill_period", value = "bill_amt", starts_with("bill")) %>%
  transmute(bill_period = bill_period, bill_amt = bill_amt)

table3 <- credit_c.d %>%
  gather(key = "repay_period", value = "status", starts_with("repay")) %>%
  transmute(limit_bal = limit_bal, sex = sex, educ = educ, marriage = marriage, age = age, repay_period = repay_period, status = status, default_nm=default_nm)

credit_gather <- data.frame(table3, table1, table2)

 # renaming file to credit default time series
credit_c.d_ts <- credit_gather %>% mutate(ID = seq.int(nrow(credit_gather)))

```


## additional data mining and data Splitting

```{r, eval=F}

# checking for missing values
lapply(credit_c.d_ts, function(x) {sum(is.na(x))})

# creating the training data
set.seed(1)
credit_train <- credit_c.d_ts %>% sample_n(100000)

set.seed(2)
credit <- anti_join(credit_c.d_ts, credit_train, by = "ID") %>%
  mutate(sex=as.factor(sex), educ=as.factor(educ), marriage=as.factor(marriage), repay_period=as.factor(repay_period),
         status=factor(status,levels = c("no credit-use", "pay duly", "revolving credit","1M-delay","2M-delay", "3M-delay","4M-delay","5M-delay","6M-11M-delay")),
         pay_period=as.factor(pay_period), bill_period = as.factor(bill_period))

# creating a validation set
set.seed(3)
credit_val <- credit %>% sample_frac(1/2)


# creating a test set
set.seed(4)
credit_test <- anti_join(credit, credit_val, by = "ID")


# Making a feature plots for our continous variables
library(mlbench)
featurePlot(x = credit_train[,c(1,5,10,12)], 
            y = credit_train$default_nm, 
            plot = "strip",
            span = .5,
            layout = c(3, 1))


# checking the correlation of our variables 
cor(credit_train[c(1,10,12)])
# our quantitative variables seem to suggest low correlation, which is good.

# transforming categorical variables to factors
credit_train <- credit_train %>% 
  mutate(sex=as.factor(sex), educ=as.factor(educ), marriage=as.factor(marriage), repay_period=as.factor(repay_period),
         status=factor(status,levels = c("no credit-use", "pay duly", "revolving credit","1M-delay","2M-delay", "3M-delay","4M-delay","5M-delay","6M-11M-delay")),
         pay_period=as.factor(pay_period), bill_period = as.factor(bill_period))
# checking for missing values
lapply(credit_train, function(x) {sum(is.na(x))})

```

I created three datasets: One for training, one for validation and another testing. However, both the validation and the test dataset are pretty much just hold-out to measure the robustness of training's prediction agaisnt two identical datasets it has never seen. 


### Classification models for the entire data

## Logistic regression

```{r, eval=F}


credit_glm <- glm(default_nm~. , data =credit_train[,-13],family = "binomial" )
summary(credit_glm)

# The logistic regression seems to make sense; however, there are some variables that are clearly insignificant

set.seed(5)
credit_probs <- predict(credit_glm, type = "response")

# quick glace on our prediction
credit_probs[1:10]

# Predicting default
cred_df_pred <- rep("current", 100000)
cred_df_pred[credit_probs >.55] <- "default"

# viewing our training error
table(cred_df_pred, credit_train$default_nm)
train_error <- (matrix[2,1]+matrix[1,2]) /100000
train_error

# Our training error seems pretty good. 

# checking against the validation set 
set.seed(6)
credit_probs <- predict(credit_glm, credit_val[,-8], type = "response" )
credit_probs[1:10]

cred_df_pred <-rep("current", 40000)
cred_df_pred[credit_probs >.55] <-"default"

# calculating the validation error
table(cred_df_pred, credit_val$default_nm)
val_error <- (matrix[2,1]+matrix[1,2]) /40000
val_error


# calculating the test error
set.seed(7)
credit_probs <- predict(credit_glm, credit_test[,-8], type = "response")
credit_probs[1:10]

cred_df_pred <- rep("current", 40000)
cred_df_pred[credit_probs >.55] <- "default"

table(cred_df_pred, credit_test$default_nm)

test_error <- (matrix[2,1]+matrix[1,2]) /40000
test_error


# Excluding bill and pay period because they are not significant. 

credit_glm <- glm(default_nm~. , data =credit_train[,-c(9, 11,13)],family = "binomial" )
summary(credit_glm)

# predicting class
set.seed(8)
credit_probs <- predict(credit_glm, type = "response")
cred_df_pred <- rep("current", 100000)
cred_df_pred[credit_probs >.55] <- "default"

matrix <- table(cred_df_pred, credit_train$default_nm)
train_error <- (matrix[2,1]+matrix[1,2]) /100000
train_error

# measuring the validation error-rate
set.seed(9)
credit_probs_1 <- predict(credit_glm, newdata = credit_val[,-8], type = "response")
cred_df_pred_1 <-rep("current", 40000)
cred_df_pred_1[credit_probs_1 >.55] <-"default"
matrix <- table(cred_df_pred_1, credit_val$default_nm)
val_error <- (matrix[2,1]+matrix[1,2]) /40000
val_error

# measuring test error
set.seed(10)
credit_probs_2 <- predict(credit_glm, newdata = credit_test[,-8], type = "response")
cred_df_pred_1[credit_probs_2 >.55] <-"default"
matrix <- table(cred_df_pred_1, credit_test$default_nm)
test_error <- (matrix[2,1]+matrix[1,2]) /40000
test_error


# Running another logistic regression with some nonlinear terms based on theoretical anticipation. 

mod_credit_glm <- glm(default_nm~I(limit_bal^2)+sex+educ+marriage+I(1/age)+repay_period+status+pay_period+I(pay_amt^2)+bill_period+I(bill_amt^2), data =credit_train[,-13],family = "binomial" )

summary(mod_credit_glm)

# making prediction
set.seed(11)
mod_credit_probs <- predict(mod_credit_glm, type = "response")

# glacing on our prediction
mod_credit_probs[1:10]

# Predicting default
mod_cred_pred <- rep("current", 100000)
mod_cred_pred[mod_credit_probs >.55] <- "default"

# viewing our training error
matrix <- table(mod_cred_pred, credit_train$default_nm)
train_error <- (matrix[2,1]+matrix[1,2])/100000
train_error


# measuring the validation error-rate
set.seed(12)
mod_credit_probs_1 <- predict(mod_credit_glm, credit_val[,-8], type = "response")

mod_cred_pred_1 <-rep("current", 40000)
mod_cred_pred_1[mod_credit_probs_1 >.55] <-"default"
matrix <- table(mod_cred_pred_1, credit_val$default_nm)
val_error <-(matrix[2,1]+matrix[1,2]) /40000
val_error

# measuring test error
set.seed(13)
mod_credit_probs_2 <- predict(mod_credit_glm, credit_test[,-8], type = "response")
mod_cred_pred_1[mod_credit_probs_2>.55] <- "default"
matrix <- table(mod_cred_pred_1, credit_test$default_nm)
test_error <- (matrix[2,1]+matrix[1,2]) /40000
test_error

```

On average, the validation and the test error rate were higher than that of the training error. hOwever, the error rate were pretty close to the training error rate. Further statistical tests would have been needed to say with confidence that the difference of the error rate accross the different dataset were not significant meaning "pretty close."

## Boosting

Please do not run this because it takes a while, and it requires a powerful computer. I am currently working on this to avoid some redundancy in tuning the parameters.

```{r, eval = F}

gbmGrid <- expand.grid(.interaction.depth = seq(1,11, by =2), .n.trees =seq(1000, 10000, by =1000), .shrinkage = c(0.001, 0.01), .n.minobsinnode = 10)
set.seed(14)
gbm_fit <- train(default_nm ~., data= credit_train[,-13], method = "gbm", tuneGrid = gbmGrid, verbose = FALSE)

```


You can certainly run the boosting below. It should more computationally friendly. 

```{r}

set.seed(15)
credit_train_1 <- credit_train %>% mutate(default_nm = factor(default_nm))

credit_train_1$default_nm <- ifelse(credit_train_1$default_nm==0,0,1)
credit_train_bst <- gbm(default_nm~., data=credit_train_1[,-13], distribution = "bernoulli", n.trees = 2000, interaction.depth = 3, shrinkage =0.2, verbose = F )
summary(credit_train_bst)

# Visualizatiion
par(mfrow= c(2,2))
plot(credit_train_bst, i = "status")
plot(credit_train_bst, i = "bill_amt")
plot(credit_train_bst, i = "limit_bal")
plot(credit_train_bst, i = "pay_amt")

credit_pred_bst <- predict(credit_train_bst, newdata = credit_val[,-8], n.trees = , type = "response")
credit_pred_bst


gbm.class<-ifelse(credit_pred_bst<0.55,'no','yes')

matrix <- table(gbm.class, credit_val$default_nm)
matrix

val_error <- (matrix[2,1]+matrix[1,2])/40000
val_error

# Testing on Test data

credit_pred_bst <- predict(credit_train_bst, newdata = credit_test[,-8], n.trees =2000 , type = "response")
credit_pred_bst


gbm.class<-ifelse(credit_pred_bst<0.55,'no','yes')

matrix <- table(gbm.class, credit_test$default_nm)
matrix

# test  error
test_error <- (matrix[2,1]+matrix[1,2])/40000
test_error

```

### RandomForest

As with booting, I am also working on a randomForest code that should avoid some redundancy in determining the optimal tuning parameters. However, I decided not to include it because it just making this file bigger.

```{r, eval=F}

credit_train_rf <- randomForest(default_nm~. , data = credit_train[,-13], mtyr= 3, ntree =500)
credit_train_rf
credit_predict <- predict(credit_train_rf, newdata = credit_val[, -8], type = "response")
matrix <- table(credit_predict, credit_val$default_nm)
val_error <-(matrix[2,1]+matrix[1,2])/40000
val_error

# visualization
importance(credit_train_rf)
varImpPlot(credit_train_rf)


# Testing on test set.

credit_predict <- predict(credit_train_rf, newdata = credit_test[, -8], type = "response")
matrix <- table(credit_predict, credit_test$default_nm)
test_error <-(matrix[2,1]+matrix[1,2])/40000
test_error

```


#### A closer look on college and graduate school students.

Filtering data to include only graduate and college students since they closely mirror the closest my most important study group. I am interested in predicting defaulting for small business men in central, east Africa. I believe that unstable income streams and inconsistent repayment behavior are the two characteristics that students share with small business men in central and east Africa. 


### Data mining and visualization

```{r}
student_univ <- credit_c.d_ts %>%
  filter(educ == c("grad school", "university"), age == c(22, 45))

# Looking at how the distribution of some variables changed (may or may not) because of our filtering

student_univ %>%
  ggplot(aes(x=limit_bal)) + geom_histogram()

student_univ %>%
  ggplot(aes(x = pay_amt)) + geom_histogram()

student_univ %>%
  ggplot(aes(x =bill_amt)) + geom_histogram()

```


### Classification models

Because the new dataset has been really reduced, I used cross-validation as well to optimize our model prediction performance.

## RandonForest

I would also suggest to not run this unless you have a fast computer. They should all be working,however.

```{r, eval=F}

#creating a model with default paramters
control <- trainControl(method = "repeatedcv", number = 5, repeats = 3)
set.seed(15)
metric <- "Accuracy"
mtry <- sqrt(ncol(student_univ[,-c(8,13)]))
tunegrid <- expand.grid(.mtry = mtry)
rf_default <- train(default_nm~., data = student_univ, method = "rf", metric = metric, tuneGrid = tunegrid, trControl = control)
print(rf_default)

control <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
set.seed(16)
metric <- "Accuracy"
mtry <- sqrt(ncol(student_univ[,-c(8,13)]))
tunegrid <- expand.grid(.mtry = mtry)
rf_default <- train(default_nm~., data = student_univ, method = "rf", metric = metric, tuneGrid = tunegrid, trControl = control)
print(rf_default)


# Random search for mtry within a range
control <- trainControl(method="repeatedcv", number=5, repeats=3, search="random")
set.seed(17)
mtry <- sqrt(ncol(student_univ[,-c(8,13)]))
rf_random <- train(default_nm~., data=student_univ, method="rf", metric=metric, tuneLength=15, trControl=control)
print(rf_random)
plot(rf_random)

control <- trainControl(method="repeatedcv", number=10, repeats=3, search="random")
set.seed(18)
mtry <- sqrt(ncol(student_univ[,-c(8,13)]))
rf_random <- train(default_nm~., data=student_univ, method="rf", metric=metric, tuneLength=15, trControl=control)
print(rf_random)
plot(rf_random)

# linear grid search
control <- trainControl(method="repeatedcv", number=5, repeats=3, search="grid")
set.seed(19)
tunegrid <- expand.grid(.mtry=c(1:11))
rf_gridsearch <- train(default_nm~., data=student_univ, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)

control <- trainControl(method="repeatedcv", number=10, repeats=3, search="grid")
set.seed(20)
tunegrid <- expand.grid(.mtry=c(1:11))
rf_gridsearch <- train(default_nm~., data=student_univ, method="rf", metric=metric, tuneGrid=tunegrid, trControl=control)
print(rf_gridsearch)
plot(rf_gridsearch)

# Manual search. Also we will stick to 10K-fold because they seem to provide a better estimate. 
control <- trainControl(method="repeatedcv", number=10, repeats = 3, search = "grid")
tunegrid <- expand.grid(.mtry=c(sqrt(ncol(student_univ[,-c(8,13)]))))
modellist <- list()
for(ntree in c(200, 500, 1000, 2000)) {
  set.seed(21)
  rf_manual <- train(default_nm~., data = student_univ, method = "rf", metric = metric, tunegrid = tunegrid, trControl= control, ntree = ntree)
  key <- toString(ntree)
  modellist[[key]] <- rf_manual
}

# compare results

results <- resamples(modellist)
summary(results)
dotplot(results)

# tuning multiple parameters

x <- student_univ[,-c(8,13)]
y <- student_univ[,8]

customRF <- list(type = "Classification", library = "randomForest", loop = NULL)
customRF$parameters <- data.frame(parameter = c("mtry", "ntree"), class = rep("numeric", 2), label = c("mtry", "ntree"))
customRF$grid <- function(x, y, len = NULL, search = "grid") {}
customRF$fit <- function(x, y, wts, param, lev, last, weights, classProbs, ...) {
  randomForest(x, y, mtry = param$mtry, ntree=param$ntree, ...)
}
customRF$predict <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata)
customRF$prob <- function(modelFit, newdata, preProc = NULL, submodels = NULL)
  predict(modelFit, newdata, type = "prob")
customRF$sort <- function(x) x[order(x[,1]),]
customRF$levels <- function(x) x$classes

# training a model
control <- trainControl(method="repeatedcv", number=10, repeats=3)
tunegrid <- expand.grid(.mtry=c(1:11), .ntree=c(500, 1000, 1500, 2000))
set.seed(22)
custom <- train(default_nm~., data=student_univ, method=customRF, metric=metric, tuneGrid=tunegrid, trControl=control)
custom$results
summary(custom)
plot(custom)

```

### Boosting

```{r, eval=F}

#Selecting the best boosting tuning paramter
fit_control <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
set.seed(23)
best_boost_1 <- train(default_nm~., data =student_univ[,-13] , method = "gbm", trControl = fit_control, verbose = FALSE)
best_boost_1

summary(best_boost_1)


# random searching 
fit_control <- trainControl(method = "repeatedcv", number = 10, repeats = 5, search = "random")
set.seed(24)
best_boost <- train(default_nm~., data =student_univ[,-13] , method = "gbm", trControl = fit_control, verbose = FALSE)
best_boost
summary(best_boost)
plot(best_boost)

fit_control <- trainControl(method = "repeatedcv", number = 10, repeats = 10, search = "random")
set.seed(25)
best_boost_2 <- train(default_nm~., data =student_univ[,-13] , method = "gbm", trControl = fit_control, verbose = FALSE)
best_boost_2
summary(best_boost_2)
plot(best_boost_2)


# Defining a tuning grid to include multiple parameters
fit_control <- trainControl(method = "repeatedcv", number = 10, repeats = 10, search = "grid")
gbmGrid <- expand.grid(.interaction.depth = seq(1,9, by =2), .n.trees =seq(100, 2000, by =50), .shrinkage = c(0.001, 0.01), .n.minobsinnode = 10)
set.seed(26)
best_boost_3 <- train(default_nm ~., data= student_univ[,-13], method = "gbm", trControl = fit_control, tuneGrid = gbmGrid, verbose = FALSE)

summary(best_boost_3)
plot(best_boost_3)


# Adjusting the parameters since more trees under the 0.01 shrinkage parameters seems to increase accurary

fit_control <- trainControl(method = "repeatedcv", number = 10, repeats = 10, search = "grid")
gbmGrid <- expand.grid(.interaction.depth = seq(3,9, by =2), .n.trees =seq(1500, 3000, by =150), .shrinkage = c(0.01, 0.1), .n.minobsinnode = 10)
set.seed(27)
best_boost_4 <- train(default_nm ~., data= student_univ[,-13], method = "gbm", trControl = fit_control, tuneGrid = gbmGrid, verbose = FALSE)
View(best_boost_4$results)

summary(best_boost_4)
plot(best_boost_4)

```

### Logistic Regression

```{r, eval=F}

fit_control <- trainControl(method = "repeatedcv", number = 10, repeats = 5)
seet.seed(28)
glm_fit <- train(default_nm~., data = student_univ[,-13], method = "glm", trControl = fit_control)
glm_fit

# adding a nonlinear variable
modified_data <- student_univ %>% 
  transmute(limit_bal=(limit_bal^2), sex = sex, educ= educ, marriage = marriage, age = I(1/age),
            repay_period = repay_period, status = status, default_nm = default_nm, pay_period = pay_period,
            pay_amt = I(pay_amt^2), bill_period = bill_period, bill_amt = I(bill_amt^2), ID = ID)

set.seed(29)
glm_fit_2 <- train(default_nm~., data = modified_data[,-13], method = "glm", trControl = fit_control)
glm_fit_2

```
 